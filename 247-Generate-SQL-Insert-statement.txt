'''
Generate SQL to handle multiple records in a CSV file
it can handle multiple records in csv.
It extracts the data and  creates a SQL Insert statement assuming the headers of 
the CSV file as the column names and the data to be the values to be used in  
SQL INSERT statement
'''
import csv

def csv_to_bulk_sql_insert(csv_file_path, table_name):
    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile)
        headers = next(reader)
        columns = ', '.join([f"`{col}`" for col in headers])

        values_list = []
        for row in reader:
            values = ', '.join([f"'{value.replace('\'', '\'\'')}'" for value in row])
            values_list.append(f"({values})")

        all_values = ',\n'.join(values_list)
        insert_statement = f"INSERT INTO {table_name} ({columns}) VALUES\n{all_values};"
        return insert_statement


# Example usage:
csv_path = r"C:\Daya\OptimalSQL-PC-folder\Python-code\sql-test-data-Product.csv"
table = "your_table_name"
sql_statement = csv_to_bulk_sql_insert(csv_path, table)

# Print the single bulk insert statement
print(sql_statement)
