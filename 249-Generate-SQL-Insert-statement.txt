'''
Generate SQL to handle multiple records in a CSV comma delimted file
and write it to a file.
it can handle multiple records in csv.
It extracts the data and  creates a SQL Insert statement assuming the headers of 
the CSV file as the column names and the data to be the values to be used in  
SQL INSERT statement
'''
import csv

def csv_to_bulk_sql_insert(csv_file_path, table_name, output_file_path):
    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        headers = next(reader)
        columns = ', '.join([col.strip() for col in headers])  # No quotes around column names

        values_list = []
        for row in reader:
            cleaned_row = [value.strip().replace("'", "''") for value in row]  # Escape single quotes
            values = ', '.join([f"'{value}'" for value in cleaned_row])
            values_list.append(f"({values})")

        all_values = ',\n'.join(values_list)
        insert_statement = f"INSERT INTO {table_name} ({columns}) VALUES\n{all_values};"

    # Write to output.sql file
    with open(output_file_path, 'w', encoding='utf-8') as f:
        f.write(insert_statement)
        print(f"SQL insert statement written to: {output_file_path}")


# Example usage:
csv_path = r"C:\Daya\OptimalSQL-PC-folder\Python-code\sql-test-data-Product.csv"
output_sql_path = r"C:\Daya\OptimalSQL-PC-folder\Python-code\output.sql"
table = "your_table_name"

csv_to_bulk_sql_insert(csv_path, table, output_sql_path)

